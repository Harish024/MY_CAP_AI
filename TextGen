import numpy
import sys
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense,Dropout, LSTM
import np_utils

#from keras.callbacks import ModelCheakpoint
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
nltk.download('stopwords')

file = open("frankenstein-2.txt").read()

def tokenize_words(input):
  input = input.lower()
  tokenizer = RegexpTokenizer(r'\w+')
  tokens = tokenizer.tokenize(input)
  filtered = filter(lambda token : token not in stopwords.words('english'),tokens)
  return "".join(filtered)
processed_inputs = tokenize_words(file)


chars = sorted(list(set(processed_inputs)))
char_to_nums = dict((c,i) for i,c in enumerate(chars))

input_len = len(processed_inputs)
vocab_len = len(chars)
print("Total number of characters : ", input_len)
print("Total Vocab : ", vocab_len)


seq_lenght = 100
x_data = []
y_data = []



for i in range(0, input_len-seq_lenght, 1):
  in_seq = processed_inputs[i:i + seq_lenght]
  out_seq = processed_inputs[i + seq_lenght]
  x_data.append([char_to_nums[char] for char in in_seq])
  y_data.append(char_to_nums[out_seq])
n_patterns = len(x_data)
print("Total Patterns : ", n_patterns)



import numpy as np
x = np.reshape(x_data,(n_patterns,seq_lenght,1))
x = x/float(vocab_len)


# one-hot encoding
from keras import utils as np_utils
y = np_utils.to_categorical(y_data)


model  = Sequential()
model.add(LSTM(256, input_shape = (x.shape[1], x.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256, return_sequences= True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation = 'softmax'))


model.compile(loss = 'categorical_crossentropy', optimizer='adam')


filepath = "model_weight_saved.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only = True, mode = 'min')
desired_callbacks = [checkpoint]

# Fitting the model and Training
model.fit(x,y, epochs = 47, batch_size = 256, callbacks = desired_callbacks)


filename = 'model_weight_saved.hdf5'
model.load_weights(filename)
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')


num_to_char = dict((i,c) for i,c in enumerate(chars))


start = np.random.randint(0, (len(x_data) - 1))
pattern = x_data[start]
print('Random Seed: ')
print("\"",''.join([num_to_char[value] for value in pattern]),"", "\"")



for i in range(1000):
    x = np.reshape(pattern, (1,len(pattern), 1))
    x = x/float(vocab_len)
    prediction = model.predict(x, verbose = 0)
    index = np.argmax(prediction)
    result = num_to_char[index]
    seq_in = [num_to_char[value] for value in pattern]
    sys.stdout.write(result)
    pattern.append(index)
    pattern = pattern[1:len(pattern)]
